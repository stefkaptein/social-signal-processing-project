
Automatically generated prosodic information
--------------------------------------------

Authored by: Gabriel Murray, University of Edinburgh
Added to the corpus by: Jonathan Kilgour
Date: 16/02/2005
Details: See information below by the author

There is prosodic information for 64 of the 75 observations:
 
Bed002 Bed003 Bed004 Bed005 Bed006 Bed008 Bed009 Bed010 Bed011 Bed012
Bed013 Bed014 Bed015 Bed016 Bed017 Bmr001 Bmr002 Bmr005 Bmr007 Bmr009
Bmr010 Bmr011 Bmr012 Bmr013 Bmr014 Bmr018 Bmr019 Bmr021 Bmr022 Bmr024
Bmr025 Bmr026 Bmr027 Bmr029 Bns001 Bns002 Bns003 Bro003 Bro004 Bro005
Bro007 Bro008 Bro010 Bro011 Bro012 Bro013 Bro014 Bro015 Bro016 Bro017
Bro018 Bro019 Bro021 Bro022 Bro023 Bro024 Bro025 Bro026 Bro027 Bro028
Bsr001 Btr001 Btr002 Buw001


Using this data in NXT
----------------------

To use prosodic data, add this to the 'agent-codings'
section of your metadata file (normally called ICSI-metadata.xml):

            <!-- prosody courtesy of Gabriel Murray, Edinburgh -->
            <coding-file name="prosody" path="../Contributions/AutomaticProsody">
                <structural-layer name="prosody-layer" points-to="words-layer">
                    <code name="prosody">
                        <attribute name="f0_mean" value-type="number"/>
                        <attribute name="f0_std" value-type="number"/>
                        <attribute name="energy" value-type="number"/>
                        <attribute name="duration" value-type="number"/>
                        <attribute name="tfidf" value-type="number"/>
                    </code>
                </structural-layer>
            </coding-file>


An example query might be 

 ($p prosody)($w w):$p@f0_mean>'1.7' && $p^$w


Details by the author
---------------------

There are five word-level features here, four of them prosodic and one
lexical.The four prosodic features are F0 mean, F0 standard deviation,
energy, and duration. The lexical feature is term-frequency/inverse
document-frequency (TFIDF).

Some of the prosodic features are normalized because they were
speaker-, meeting-, or word-dependent (or a combination). For example,
the F0 mean is normalized by the given speaker's F0 baseline for that
particular meeting, since an F0 range is completely
speaker-dependent. I did not deem it necessary to normalize F0
standard deviation. The duration feature is word-dependent, since some
words are inherently longer than others. This was normalized
paradigmatically, i.e. by comparison with other instances of the
word. This method leaves a little bit to be desired, as a sparse
number of tokens for a given word type will be problematic when making
this comparison (I have also done duration normalization by expected
duration, and I will try to make this available soon). Energy is
normalized in a couple of ways. First of all, it is normalized by
speaker by meeting, and secondly it is normalized paradigmatically by
word, like duration was. The reason for this latter normalization is
that some words have phones that are inherently higher energy.

Though I call the lexical feature TFIDF, it is actually a bit
different than this common metric. Whereas the "IDF" part of TFIDF is
not sensitive to the frequency of word occurrence in other documents
(it only sees how many other documents contain the word), I wanted a
direct comparison of word frequency within a document compared to
frequency across all documents. So, if a word hasthe same frequency in
one document as it has across all the documents, the score will be
1. If the frequency is much higher in one document than across
documents, the lexical score will be higher.

